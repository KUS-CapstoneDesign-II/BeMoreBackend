<!--
ì¹´ë©”ë¼ + ìŒì„±(STT) + ì–¼êµ´ ëœë“œë§ˆí¬ í†µí•© í…ŒìŠ¤íŠ¸ìš© í”„ë¡ íŠ¸ì—”ë“œ
-->

<!DOCTYPE html>
<html lang="ko">
  <head>
    <meta charset="UTF-8" />
    <title>Emotion Analyzer Demo</title>
    <style>
      video,
      canvas {
        position: absolute;
        top: 0;
        left: 0;
        transform: scaleX(-1);
      }

      #status {
        position: fixed;
        top: 10px;
        left: 10px;
        background: rgba(0, 0, 0, 0.7);
        color: white;
        padding: 10px;
        border-radius: 8px;
        font-family: sans-serif;
        z-index: 10;
      }
    </style>
  </head>
  <body>
    <div id="status">ğŸ¥ ì´ˆê¸°í™” ì¤‘...</div>
    <video id="input_video" autoplay playsinline></video>
    <canvas id="output_canvas"></canvas>

    <!-- Mediapipe ë¼ì´ë¸ŒëŸ¬ë¦¬ -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>

    <script>
      const statusDiv = document.getElementById("status");
      const videoElement = document.getElementById("input_video");
      const canvasElement = document.getElementById("output_canvas");
      const canvasCtx = canvasElement.getContext("2d");

      // ====== WebSocket ì—°ê²° ======
      const ws = new WebSocket("ws://localhost:8000");
      ws.onopen = () => (statusDiv.textContent = "âœ… WebSocket ì—°ê²° ì™„ë£Œ");
      ws.onmessage = (msg) => {
        try {
          const data = JSON.parse(msg.data);
          if (data.emotion) {
            statusDiv.textContent = `ğŸ˜ƒ ê°ì •: ${data.emotion}`;
          }
        } catch {}
      };

      // ====== Mediapipe FaceMesh ì„¤ì • ======
      const faceMesh = new FaceMesh({
        locateFile: (file) =>
          `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
      });

      faceMesh.setOptions({
        maxNumFaces: 1,
        refineLandmarks: true,
        minDetectionConfidence: 0.5,
        minTrackingConfidence: 0.5,
      });

      let frameCount = 0;
      faceMesh.onResults((results) => {
        canvasElement.width = videoElement.videoWidth;
        canvasElement.height = videoElement.videoHeight;
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        canvasCtx.drawImage(
          results.image,
          0,
          0,
          canvasElement.width,
          canvasElement.height
        );

        if (results.multiFaceLandmarks) {
          for (const landmarks of results.multiFaceLandmarks) {
            for (const point of landmarks) {
              const x = point.x * canvasElement.width;
              const y = point.y * canvasElement.height;
              canvasCtx.beginPath();
              canvasCtx.arc(x, y, 1.5, 0, 2 * Math.PI);
              canvasCtx.fillStyle = "red";
              canvasCtx.fill();
            }
          }

          frameCount++;
          if (frameCount % 3 === 0 && ws.readyState === WebSocket.OPEN) {
            ws.send(JSON.stringify(results.multiFaceLandmarks));
          }
        }
      });

      // ====== ì¹´ë©”ë¼ ì‹œì‘ ======
      const camera = new Camera(videoElement, {
        onFrame: async () => await faceMesh.send({ image: videoElement }),
        width: 640,
        height: 480,
      });
      camera.start();

      // ====== ë§ˆì´í¬ ë…¹ìŒ ë° STT ì „ì†¡ ======
      let mediaRecorder;
      let audioChunks = [];

      async function startMicRecording() {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

        // mimeType ëª…ì‹œ
        mediaRecorder = new MediaRecorder(stream, { mimeType: "audio/webm; codecs=opus" });

        mediaRecorder.ondataavailable = (e) => {
          if (e.data && e.data.size > 0) {
            audioChunks.push(e.data);
          }
        };

        // 5ì´ˆ ë‹¨ìœ„ë¡œ ìë™ ì „ì†¡
        setInterval(async () => {
          if (audioChunks.length === 0) return;

          const blob = new Blob(audioChunks, { type: "audio/webm; codecs=opus" });
          audioChunks = [];

          if (blob.size === 0) return; // ë¹ˆ ì˜¤ë””ì˜¤ ì „ì†¡ ë°©ì§€

          if (blob.size < 500) return; // ë„ˆë¬´ ì‘ì€ íŒŒì¼ ë¬´ì‹œ


          const formData = new FormData();
          formData.append("audio", blob, `speech_${Date.now()}.webm`);

          try {
            const res = await fetch("/api/transcribe", {
              method: "POST",
              body: formData,
            });
            const data = await res.json();
            console.log("ğŸ—£ï¸ STT ê²°ê³¼:", data.text);
          } catch (err) {
            console.error("STT ì—…ë¡œë“œ ì‹¤íŒ¨:", err);
          }
        }, 5000);

        mediaRecorder.start(1000); // 1ì´ˆ ë‹¨ìœ„ chunk
        statusDiv.textContent = "ğŸ¤ ìŒì„± ë…¹ìŒ ì¤‘ + ì–¼êµ´ ì¶”ì  ì¤‘";
      }

      startMicRecording().catch((err) => {
        statusDiv.textContent = "ğŸš« ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨: " + err.message;
      });
    </script>
  </body>
</html>
