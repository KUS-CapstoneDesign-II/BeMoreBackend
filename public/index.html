<!DOCTYPE html>
<html lang="ko">
Â 

<head>
  Â  Â 
  <meta charset="UTF-8" />
  Â  Â  <title>Emotion Analyzer Demo</title>
  Â  Â  <style>
    video,
    canvas {
      position: absolute;
      top: 0;
      left: 0;
      transform: scaleX(-1);
    }

    #status {
      position: fixed;
      top: 10px;
      left: 10px;
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 10px;
      border-radius: 8px;
      font-family: sans-serif;
      z-index: 10;
    }
  </style>
  Â 
</head>
Â 

<body>
  Â  Â  <div id="status">ğŸ¥ ì´ˆê¸°í™” ì¤‘...</div>
  Â  Â  <video id="input_video" autoplay playsinline></video>
  Â  Â  <canvas id="output_canvas"></canvas>

  Â  Â  Â  Â 
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  Â  Â 
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>

  Â  Â 
  <script>
    const statusDiv = document.getElementById("status");
    const videoElement = document.getElementById("input_video");
    const canvasElement = document.getElementById("output_canvas");
    const canvasCtx = canvasElement.getContext("2d");

    // ====== WebSocket ì—°ê²° ======
    const ws = new WebSocket("ws://localhost:8000");
    ws.onopen = () => (statusDiv.textContent = "âœ… WebSocket ì—°ê²° ì™„ë£Œ");
    ws.onmessage = (msg) => {
      try {
        const data = JSON.parse(msg.data);
        if (data.emotion) {
          statusDiv.textContent = `ğŸ˜ƒ ê°ì •: ${data.emotion}`;
        }
      } catch { }
    };

    // ====== Mediapipe FaceMesh ì„¤ì • ======
    const faceMesh = new FaceMesh({
      locateFile: (file) =>
        `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}`,
    });

    faceMesh.setOptions({
      maxNumFaces: 1,
      refineLandmarks: true,
      minDetectionConfidence: 0.5,
      minTrackingConfidence: 0.5,
    });

    let frameCount = 0;
    faceMesh.onResults((results) => {
      canvasElement.width = videoElement.videoWidth;
      canvasElement.height = videoElement.videoHeight;
      canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
      canvasCtx.drawImage(
        results.image,
        0,
        0,
        canvasElement.width,
        canvasElement.height
      );

      if (results.multiFaceLandmarks) {
        for (const landmarks of results.multiFaceLandmarks) {
          for (const point of landmarks) {
            const x = point.x * canvasElement.width;
            const y = point.y * canvasElement.height;
            canvasCtx.beginPath();
            canvasCtx.arc(x, y, 1.5, 0, 2 * Math.PI);
            canvasCtx.fillStyle = "red";
            canvasCtx.fill();
          }
        }

        frameCount++;
        if (frameCount % 3 === 0 && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify(results.multiFaceLandmarks));
        }
      }
    });

    // ====== ì¹´ë©”ë¼ ì‹œì‘ ======
    const camera = new Camera(videoElement, {
      onFrame: async () => await faceMesh.send({ image: videoElement }),
      width: 640,
      height: 480,
    });
    camera.start();

    // ====== ë§ˆì´í¬ ë…¹ìŒ ë° STT ì „ì†¡ (ìˆ˜ì •ëœ ì•ˆì •ì ì¸ ë¡œì§) ======
    let mediaRecorder;
    let audioChunks = [];
    let audioStream;
    let recordingInterval;

    // MediaRecorderë¥¼ ì´ˆê¸°í™”í•˜ê³  ì‹œì‘í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    const initAndStartRecorder = (stream) => {
      // ì´ì „ ì¸ìŠ¤í„´ìŠ¤ê°€ ìˆë‹¤ë©´ ì¤‘ì§€í•˜ê³  ì •ë¦¬ (ì•ˆì „ ì¡°ì¹˜)
      if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
      }

      audioChunks = []; // ë°ì´í„° ë²„í¼ ì´ˆê¸°í™”

      // ìƒˆë¡œìš´ MediaRecorder ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
      mediaRecorder = new MediaRecorder(stream, { mimeType: "audio/webm; codecs=opus" });

      mediaRecorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          audioChunks.push(e.data);
        }
      };

      mediaRecorder.start(); // ì¦‰ì‹œ ë…¹ìŒ ì‹œì‘
    };

    async function startMicRecording() {
      audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // ìµœì´ˆ ë…¹ìŒ ì‹œì‘
      initAndStartRecorder(audioStream);

      // 5ì´ˆ ë‹¨ìœ„ë¡œ ìë™ ì „ì†¡ (ì¤‘ì§€ -> ì „ì†¡ -> ì¬ì‹œì‘)
      recordingInterval = setInterval(async () => {

        // 1. ë…¹ìŒ ì¤‘ì§€: ondataavailable ì´ë²¤íŠ¸ë¥¼ ê°•ì œí•˜ì—¬ ì²­í¬ë¥¼ ìµœì¢… í™•ë³´
        if (mediaRecorder.state === 'recording') {
          mediaRecorder.stop();
        } else {
          // ì´ë¯¸ ë©ˆì¶°ìˆë‹¤ë©´ (ì˜ˆ: ì¤‘ì§€ ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘) ì¬ì‹œì‘ë§Œ í•˜ê³  ë°ì´í„° ì „ì†¡ì€ ìŠ¤í‚µ
          initAndStartRecorder(audioStream);
          return;
        }

        // ondataavailableì´ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ëœ í›„, audioChunksê°€ ì±„ì›Œì§€ê¸°ë¥¼ ê¸°ë‹¤ë¦¼
        // ë°ì´í„°ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸° (setTimeout ëŒ€ì‹  ë‹¤ìŒ ë£¨í”„ì—ì„œ ì²˜ë¦¬)
        // ì¦‰ì‹œ ì‹¤í–‰ë  ê²½ìš°, audioChunks.length === 0 ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë°©ì–´ ë¡œì§ ì¶”ê°€

        // 2. ë°ì´í„° ì²˜ë¦¬ ë° ì „ì†¡
        // ondataavailableì´ ì™„ì „íˆ ì‹¤í–‰ë˜ê¸°ë¥¼ ê¸°ë‹¤ë¦¬ê¸° ìœ„í•´ ì ì‹œ ë”œë ˆì´ë¥¼ ì£¼ê±°ë‚˜,
        // ë‹¤ìŒ ì¸í„°ë²Œ ë£¨í”„ì—ì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë‹¤ìŒ ë£¨í”„ë¥¼ ìœ„í•´
        // ë°”ë¡œ initAndStartRecorder(audioStream)ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šê³ , í˜„ì¬ í™•ë³´ëœ ì²­í¬ë§Œ í™•ì¸í•©ë‹ˆë‹¤.

        if (audioChunks.length === 0) {
          // ë°ì´í„°ê°€ í™•ë³´ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ë‹¤ìŒ ë£¨í”„ì—ì„œ ì¬ì‹œë„ (ì¤‘ì§€/ì¬ì‹œì‘ì€ initAndStartRecorderì—ì„œ ì²˜ë¦¬ë¨)
          initAndStartRecorder(audioStream);
          return;
        }

        const blob = new Blob(audioChunks, { type: "audio/webm; codecs=opus" });
        // audioChunks = []; // ë²„í¼ëŠ” initAndStartRecorderì—ì„œ ì´ˆê¸°í™”ë¨

        if (blob.size < 500) {
          initAndStartRecorder(audioStream); // ì‘ì€ íŒŒì¼ ë¬´ì‹œ í›„ ì¬ì‹œì‘
          return;
        }

        const formData = new FormData();
        formData.append("audio", blob, `speech_${Date.now()}.webm`);

        // 3. ë…¹ìŒ ì¬ì‹œì‘ (ì „ì†¡ ì „ì— ì‹œì‘í•˜ì—¬ ë¹ˆ ì‹œê°„ ìµœì†Œí™”)
        initAndStartRecorder(audioStream);

        // 4. STT ì „ì†¡
        try {
          const res = await fetch("/api/transcribe", {
            method: "POST",
            body: formData,
          });
          const data = await res.json();
          console.log("ğŸ—£ï¸ STT ê²°ê³¼:", data.text);
        } catch (err) {
          console.error("STT ì—…ë¡œë“œ ì‹¤íŒ¨:", err);
        }

      }, 5000);

      statusDiv.textContent = "ğŸ¤ ìŒì„± ë…¹ìŒ ì¤‘ + ì–¼êµ´ ì¶”ì  ì¤‘";
    }

    startMicRecording().catch((err) => {
      statusDiv.textContent = "ğŸš« ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨: " + err.message;
    });

    // í˜ì´ì§€ë¥¼ ë– ë‚  ë•Œ Interval ì •ë¦¬
    window.onbeforeunload = () => {
      clearInterval(recordingInterval);
      if (mediaRecorder && mediaRecorder.stream) {
        mediaRecorder.stream.getTracks().forEach(track => track.stop());
      }
    };
  </script>
  Â 
</body>

</html>